---
title: "STAT 223 Homework 2"
author: "Group 5 (Yiluan Song, Peter Trubey)"
date: "2/6/2020"
output: html_document

---

<style type="text/css">

body{ /* Normal  */
      font-size: 18px;
  }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(message  = FALSE, warnings=FALSE)
library(ltsa)
library(tidyverse)
```
## Chapter 2 Problem 19
Solve the characteristic polynomial
$$\Phi(u)=1-\phi_1u-\phi_2u^2=1-0.9u+0.9u^2=0$$
```{r}
roots<-polyroot(c(1, -0.9, 0.9))
Mod(roots)

a<-1/roots
Mod(a)
```
As the modulus of characheristic roots are greater than 1 (modulus of characheristic reciprocal roots smaller than 1), the process is stable.

A general solution to the MA coefficients after inversion are
$$\psi_j=\alpha_1^jp_1(j)+...+\alpha_r^jp_r(j)$$
, where $p_i(j)$ is a polynomial of dergee $m_i-1$.

In this case, we have two different roots, each with multiplicity 1, so the solution can be written as
$$\psi_j=\alpha_1^ja+\alpha_2^jb$$
We now need to solve for the values of $a$ and $b$ with the following initial conditions:

When j=0, $$\psi_0=\theta_0=1$$
When j=1, $$\psi_1-\phi_1\psi_0=0$$ $$\psi_1=\phi_1$$ $$\alpha_1a+\alpha_2b=-0.9$$
When j=2, $$\psi_2-\phi_1\psi_1-\phi_2\psi_0=0$$ $$\psi_2=\phi_1^2+\phi_2$$ $$\alpha_1^2a+\alpha_2^2b=(-0.9)^2-0.9$$

We get the following $a$ and $b$:
```{r psi coef}
A<-rbind(a, a^2)
b<-rbind(0.9, (0.9^2-0.9))

psi_coef<-data.frame(coefficient=solve(A, b)) %>% 
  mutate(modulus=Mod(coefficient)) %>% 
  mutate(argument=Arg(coefficient))
psi_coef
```

$$\psi_j=\alpha_1^ja+\alpha_2^jb=(+ i)^j(+ i)+(+ i)^j(+ i)$$
Visualizing some $\psi$.
```{r psi}
psi<-rep(NA, 50)
for(j in 1:50){
  psi[[j]]<-Re(a[[1]]^j*psi_coef$coefficient[[1]]+a[[2]]^j*psi_coef$coefficient[[2]])
}
plot(psi, type="l")
```

## Chapter 2 Problem 24 (a)

Raw data was first detrended using LOESS regression.
```{r}
iso<-read.table("/raid/users/ysong67/GitHub/STAT223/oxygen_isotope.txt", skip=6, header=F)
str(iso)
iso<-iso$V2
n<-length(iso)

plot(iso, type="l")
lines(fitted(loess(iso~c(1:n))), col="red")


loess(iso~c(1:n))
iso_detrend<-residuals(loess(iso~c(1:n)))
plot(iso_detrend, type="l")
```

AR Model: $y_t=\sum_{j=1}^{p}\phi_j y_{t-j}+\epsilon_t$, where $\epsilon_t\sim N(0, v)$
I consider this in the form of a linear model $\mathbf{y}=\mathbf{F}'\mathbf{\beta}+\mathbf{\epsilon}$, where $\mathbf{\epsilon}\sim N(\mathbf{0}, v\mathbf{I})$ and computed the MLE of $\beta$ and $v$ as
$$\hat{\mathbf{\beta}}=(\mathbf{F}'\mathbf{F})^{-1}\mathbf{F}\mathbf{y}\text{        (1)}$$
and
$$\hat{v}=s^2=\frac{(\mathbf{y}-\mathbf{F}'\mathbf{\beta})'(\mathbf{y}-\mathbf{F}'\mathbf{\beta})}{T-p}\text{        (2)}$$
, where $T$ is the total number of observations and $p$ is the number of parameters.

I fitted AR(1) to AR(15) models and calculated the AIC and BIC scores using the corresponsing $s^2$ following:
$$AIC=2p+(T-p)\log(s^2_p)$$
$$BIC=\log(T-p)p+(T-p)\log(s^2_p)$$
```{r fit model}

# placeholder
phi_p<-vector(mode="list", length = 15)
resid_p<-vector(mode="list", length = 15)
s_square_p<-vector(mode="list", length = 15)
AIC_p<-rep(NA, 15)
BIC_p<-rep(NA, 15)

for(p in 1:15) {
  y<-iso_detrend[(p+1):(n)]
  y_prev<-matrix(NA, ncol = n-p, nrow = p)
  for(i in 1:p) {
    y_prev[i,]<-iso_detrend[(p-i+1):(n-i)]
  }
  
  phi_p[[p]]<-solve(y_prev %*% t(y_prev))%*%y_prev%*%y
  resid_p[[p]]<-y-t(y_prev)%*% phi_p[[p]]
  s_square_p[[p]]<-(t(resid_p[[p]]) %*%(resid_p[[p]]))/(n-p)
  AIC_p[[p]]<-2*(p)+(n-p)*log(s_square_p[[p]])
  BIC_p[[p]]<-log(n-p)*p+(n-p)*log(s_square_p[[p]])
}
phi<-c(phi_p[[3]])
v<-c(s_square_p[[3]])

AICBIC<-data.frame(p=1:15,AIC=AIC_p, BIC=BIC_p)
ggplot(AICBIC)+
  geom_point(aes(x=p, y=AIC), col="red", pch="a", cex=5)+
  geom_point(aes(x=p, y=BIC), col="blue", pch="b", cex=5)+
  ylab("Score")+
  xlab("Model order")+
  theme_classic()
```
Both AIC and BIC were the lowest at model order 3. The rest of this problem will follow AR(3).

## Chapter 2 Problem 24 (b)
Using eqn 1, the three AR coefficients were estimated as follows.
```{r AR coef}
phi
```
The residuals were calculated using eqn. 2.
```{r resid}
plot(resid_p[[3]], type="l")

acf(resid_p[[3]], main="ACF")
pacf(resid_p[[3]],main="PACF")
qqnorm(resid_p[[3]])
qqline(resid_p[[3]], col="red")
```
The residuals do not completely satisfy the assumption of normality.

## Chapter 2 Problem 24 (c)
Solving for the characteristic polynomial $$\Phi(u)=1-\phi_1u-\phi_2u^2-\phi3u^3=0$$ using the the AR coefficients found in (b), we get the following reciprocal roots:
```{r root}
poly_coef<-c(1, -phi)
invroot<-data.frame(invroot=1/polyroot(poly_coef)) %>% 
  mutate(modulus=Mod(invroot)) %>% 
  mutate(argument=abs(Arg(invroot))) %>% 
  mutate(wavelength=2*pi/argument) %>% 
  arrange(desc(wavelength)) 
invroot
```
There is one pair of complex reciprocal roots with a modulus of 0.7033 and wavelength of 18.10.

## Chapter 2 Problem 24 (d)

### Method 1: Yule-Walker equations
The h-step-ahead prediction and corresponding MSE based on $y_{1:t}$ are

$$y^{t}_{t+h}=\phi^{(h)}(t, 1)y_t+...+\phi^{(h)}(t, t)y_1=(\phi^{(h)}(t, 1), ..., \phi^{(h)}(t, t))'(y_t, ..., y_1)=\mathbf{\phi}_t^{(h)}y_{t:1} \text{        (1)}$$
and

$$MSE^t_{t+h}=\gamma_0-\mathbf{\gamma'}_t^{(h)}\Gamma_t^{-1}\mathbf{\gamma}_t^{(h)} \text{        (2)}$$

with $\mathbf{\phi}_t^{(h)}$ being the solution of $$\mathbf{\Gamma}_t\mathbf{\phi}_t^{(h)}=\mathbf{\gamma}_t^{(h)}$$

, where 

$$\mathbf{\gamma}_t^{(h)}=(\gamma(h), \gamma(h+1), ..., \gamma(t+h+1))'$$ and 
$$\mathbf{\Gamma}_t=\mathbf{\Gamma}_t(1)=\{\gamma(j-h)\}^t_{j, h=1}=\left[
 \begin{matrix}
   \gamma(0) & \gamma(1) & ... & \gamma(t-1)\\
   \gamma(1) & \gamma(0) & ... & \gamma(t-2) \\
   \vdots & \vdots & \ddots& \vdots \\
   \gamma(t-1) & \gamma(t-2) & ... & \gamma(0)
  \end{matrix} 
\right]$$

These are equations (2.15) and (2.16) from Prado & West (2010).

First, calculate $\mathbf{\Gamma}_t$ using theoretical autocovariance function. I used the R package _tacvfARMA_.
```{r Gamma}
GAMMA_t<-matrix(NA, ncol=n, nrow=n)
for (j in 1:n) {
  GAMMA_t[j,c(j:n)]<-tacvfARMA(phi = phi,  maxLag = 1+n-2, sigma2 = v)[1:(n-j+1)]
  if(j>1) {
    GAMMA_t[j, c(1:(j-1))]<-rev(tacvfARMA(phi = phi,  maxLag = 1+n-2, sigma2 = v)[2:j])
  }
}
str(GAMMA_t)
```

Then calculate $\mathbf{\gamma}_t^{(h)}$ for $h$ from 1 to 100.
```{r gamma}
gamma_t<-matrix(NA, ncol=100, nrow=n)
for (h in 1:100) {
  gamma_t[,h]<-matrix(tacvfARMA(phi = phi,  maxLag = n+h-1, sigma2 = v)[(h+1):(n+h)])
}
str(gamma_t)
```

Solve for $\mathbf{\phi}_t^{(h)}$ for $h$ from 1 to 100.
```{r pacf}
pacf_t<-matrix(NA, ncol=100, nrow=n)
for (h in 1:100) {
  pacf_t[,h]<-solve(a=GAMMA_t, b=gamma_t[,h])
}
str(pacf_t)
```

A sanity check: For stationary AR(3) model, $\phi(3, 1), \phi(3, 1), \phi(3, 1)$ should correspond to the AR coefficients $\phi_1,\phi_2,\phi_3$ and decays to 0 after 3.

Recall the AR coefficients are:
```{r AR coef again}
round(phi,5)
```

When h=1, the first 10 PACF we have are:
```{r pacf check}
round(pacf_t[1:10,1],5)
```

We can now calculate $y^{t}_{t+h}$ where $h=1:100$ using eqn. 1:
```{r y_pred}
y_pred<-rep(NA, 100)
for (h in 1:100) {
    y_pred[[h]]<-t(matrix(rev(iso_detrend)))%*%matrix(pacf_t[,h])
}
plot(y_pred, type="l", ylab="Predicted", xlab="h")
```

Calculate $MSE^{t}_{t+h}$ where $h=1:100$ using eqn. 2:
```{r MSE_pred}
gamma_0<-tacvfARMA(phi = phi,  maxLag = 0, sigma2 = v)
MSE_pred<-rep(NA, 100)
for (h in 1:100) {
  MSE_pred[[h]]<-gamma_0-t(gamma_t[,h])%*%solve(GAMMA_t)%*%gamma_t[,h]
}
plot(MSE_pred, type="l", ylab="MSE", xlab="h")
```

Visualize these predictions with 95% prediction intervals. Here the 95% prediction intervals were calculated as $(y^{t}_{t+h}-1.96MSE^{t}_{t+h}, y^{t}_{t+h}+1.96MSE^{t}_{t+h})$.

```{r visualize}
pred_df<-data.frame(time=n+c(1:100),predicted=y_pred, lower=y_pred-1.96*MSE_pred, upper=y_pred+1.96*MSE_pred) 

obs_df<-data.frame(time=1:n, observed=iso_detrend)

iso_trend<-coef(lm(iso~c(1:n)))[1]+coef(lm(iso~c(1:n)))[2]*c(1:(n+100))

combine_df<-full_join(pred_df, obs_df, by="time") %>% 
  arrange(time) %>% 
  mutate(predicted_trend=predicted+iso_trend,
         lower_trend=lower+iso_trend,
         upper_trend=upper+iso_trend,
         observed_trend=observed+iso_trend)

ggplot(combine_df)+
  geom_line(aes(x=time, y=predicted), na.rm=TRUE, col="blue")+
  geom_line(aes(x=time, y=lower), na.rm=TRUE, col="blue", lty=2)+
  geom_line(aes(x=time, y=upper), na.rm=TRUE, col="blue", lty=2)+
  xlim(n+1,n+100)+
  ylab("Oxygen isotope")+
  xlab("Timestep")+
  theme_classic()
```

Visualize predicted values together with previously observed values.

```{r visualize 2}
ggplot(combine_df)+
  geom_line(aes(x=time, y=observed), na.rm=TRUE)+
  geom_line(aes(x=time, y=predicted), na.rm=TRUE, col="blue")+
  geom_line(aes(x=time, y=lower), na.rm=TRUE, col="blue", lty=2)+
  geom_line(aes(x=time, y=upper), na.rm=TRUE, col="blue", lty=2)+
  xlim(1,n+100)+
  ylab("Oxygen isotope")+
  xlab("Timestep")+
  theme_classic()
```

### Method 2: Innovations algorithm (Brockwell and Davis, 1991)
Alternatively, we can calculate the $y^{t}_{t+h}$ and $MSE^{t}_{t+h}$ recursively, using the following equations.
$$y^{t}_{t+h}=\sum_{j=h}^{t+h-1}b_{t+h-1, j}(y_{t+h-j}-y_{t+h-j}^{t+h-j-1})\text{        (3)}$$
and
$$MSE^{t}_{t+h}=\gamma(0)-\sum_{j=h}^{t+h-1}b_{t+h-1, j}^2 MSE_{t+h-j}^{t+h-j-1}\text{        (4)}$$
, for $t\ge1$ where, for $j=0:(t-1)$
$$b_{t+h-1,t+h-1-j}=\frac{\gamma(t+h-1-j)-\sum_{l=0}^{j-1}b_{j, j-l} b_{t+h-1, t+h-1-l} MSE^l_{l+1}} {MSE^j_{j+1}}$$
The algorithm is initialized at $y^0_1=0$ and $MSE^0_1=\gamma(0)$.

The general workflow of this code is to 1) update all $b$, starting from $b_{t+h-1,t+h-1}$, ending at $b_{t+h-1,h}$; 2) update all $y^t_{t+h}$ using eqn. 3; 3) update all $MSE^{t}_{t+h}$ using eqn. 4. Note that calculations for $h=1$ needs to be done first before calculating for other $h$ in parallel.

The calculation of $b$ was fairly slow, so I used parallel computing and stored the output into a csv file. The code for innovations algorithm is included in this R markdown file.
```{r inno, eval=FALSE}
# placeholders
y_forecast<-matrix(NA, nrow=n+100+1, ncol=n+100+1)
y_new<-rep(NA, n+100+1)
MSE_forecast<-matrix(NA, nrow=n+100+1, ncol=n+100+1)
b<-matrix(NA, nrow=n+100+1, ncol=n+100+1)

# initialize
y_forecast[1, 2]<-0
MSE_forecast[1, 2]<-tacvfARMA(phi = phi,  maxLag = 0, sigma2 = v)
y_new[2:(n+1)]<-iso_detrend


# calculate y and MSE for h=1
h<-1
t<-1
while (t<=n) {
  # update b
  for (j in 0:(t+h-1)) {
    gamma<-tacvfARMA(phi = phi,  maxLag = t+h-1-j, sigma2 = v)[(t+h-1-j+1)]
    b[(t+h-1+1),(t+h-1-j+1) ]<-gamma
    if (j>0) {
      for (l in 0:(j-1)) {
        b[(t+h-1+1),(t+h-1-j+1) ]<-b[(t+h-1+1),(t+h-1-j+1) ]-b[(j+1), (j-l+1)]*b[(t+h-1+1), (t+h-1-l+1)]*MSE_forecast[(l+1), (l+1+1)]
      }
    }
    b[(t+h-1+1),(t+h-1-j+1) ]<-b[(t+h-1+1),(t+h-1-j+1) ]/MSE_forecast[(j+1), (j+1+1)] 
  }
  
  # update y_forecast
  y_forecast[(t+1),(t+h+1)]<-0
  for (j in h:(t+h-1)){
    y_forecast[(t+1),(t+h+1)]<-y_forecast[(t+1),(t+h+1)]+b[(t+h-1+1), (j+1)]*(y_new[t+h-j+1]-y_forecast[(t+h-j-1+1),(t+h-j+1)])
  }
  
  # update MSE_forecast
  MSE_forecast[(t+1),(t+h+1)]<-tacvfARMA(phi = phi,  maxLag = 0, sigma2 = v)
  for (j in h:(t+h-1)) {
    MSE_forecast[(t+1),(t+h+1)]<-MSE_forecast[(t+1),(t+h+1)]-(b[(t+h-1+1),(j+1)])^2*MSE_forecast[(t+h-j-1+1), (t+h-j+1)]
  }
  t<-t+1
}

# set up parallel computing
library(doParallel)
library(foreach)

cores<-detectCores()
cl <- makeCluster(cores[1]-1) #not to overload your computer
registerDoParallel(cl)


# calculate y and MSE for all h
inno_pred<-
  foreach (h=1:100, .combine=rbind) %dopar% {
    library(ltsa)
    t<-1
    while (t<=n) {
      # update b
      for (j in 0:(t+h-1)) {
        gamma<-tacvfARMA(phi = phi,  maxLag = t+h-1-j, sigma2 = v)[(t+h-1-j+1)]
        b[(t+h-1+1),(t+h-1-j+1) ]<-gamma
        if (j>0) {
          for (l in 0:(j-1)) {
            b[(t+h-1+1),(t+h-1-j+1) ]<-b[(t+h-1+1),(t+h-1-j+1) ]-b[(j+1), (j-l+1)]*b[(t+h-1+1), (t+h-1-l+1)]*MSE_forecast[(l+1), (l+1+1)]
          }
        }
        b[(t+h-1+1),(t+h-1-j+1) ]<-b[(t+h-1+1),(t+h-1-j+1) ]/MSE_forecast[(j+1), (j+1+1)] 
      }
      
      # update y_forecast
      y_forecast[(t+1),(t+h+1)]<-0
      for (j in h:(t+h-1)){
        y_forecast[(t+1),(t+h+1)]<-y_forecast[(t+1),(t+h+1)]+b[(t+h-1+1), (j+1)]*(y_new[t+h-j+1]-y_forecast[(t+h-j-1+1),(t+h-j+1)])
      }
      
      # update MSE_forecast
      MSE_forecast[(t+1),(t+h+1)]<-tacvfARMA(phi = phi,  maxLag = 0, sigma2 = v)
      for (j in h:(t+h-1)) {
        MSE_forecast[(t+1),(t+h+1)]<-MSE_forecast[(t+1),(t+h+1)]-(b[(t+h-1+1),(j+1)])^2*MSE_forecast[(t+h-j-1+1), (t+h-j+1)]
      }
      t<-t+1
    }
    c(h, y_forecast[n+1, n+h+1],MSE_forecast[n+1, n+h+1])
  }
# stop parallel computing
stopCluster(cl)

# save output
colnames(inno_pred)<-c("h", "y_pred", "MSE_pred")
write_csv(data.frame(inno_pred), "/raid/users/ysong67/GitHub/STAT223/inno_pred.csv")
```

The results from this method are exactly the same as those from method 1. Here I show the first 10 sets of output.

```{r compare}
inno_pred<-read_csv("/raid/users/ysong67/GitHub/STAT223/inno_pred.csv")

compare_df<-data.frame(y_pred_Method_1=y_pred, MSE_pred_Method_1=MSE_pred, y_pred_Method_2=inno_pred$y_pred, MSE_pred_Method_2=inno_pred$MSE_pred)
compare_df[1:10,]
```


